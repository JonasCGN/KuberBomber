# Sistema de SimulaÃ§Ã£o de Disponibilidade Kubernetes

## ğŸ¯ VisÃ£o Geral

Sistema completo de simulaÃ§Ã£o de disponibilidade para infraestrutura Kubernetes, implementando:

- **DistribuiÃ§Ã£o Exponencial**: Falhas baseadas em MTTF (Mean Time To Failure)
- **Timing HÃ­brido**: 1 minuto real entre falhas + tempo real de recuperaÃ§Ã£o
- **IntegraÃ§Ã£o Kubernetes**: Uso de `kubectl` para falhas e monitoramento reais
- **RelatÃ³rios Detalhados**: CSV com eventos, estatÃ­sticas e mÃ©tricas de disponibilidade

## ğŸ—ï¸ Arquitetura

```
kuber_bomber/
â”œâ”€â”€ simulation/
â”‚   â””â”€â”€ availability_simulator.py    # Motor principal de simulaÃ§Ã£o
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ availability_cli.py          # Interface de linha de comando
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ health_checker.py            # Monitoramento de saÃºde dos componentes
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ csv_reporter.py              # GeraÃ§Ã£o de relatÃ³rios CSV
â””â”€â”€ failure_injectors/
    â”œâ”€â”€ pod_injector.py              # InjeÃ§Ã£o de falhas em pods
    â”œâ”€â”€ node_injector.py             # InjeÃ§Ã£o de falhas em nodes
    â””â”€â”€ control_plane_injector.py    # InjeÃ§Ã£o de falhas no control plane
```

## âš™ï¸ Componentes Configurados

| Componente | Tipo | MTTF | DescriÃ§Ã£o |
|------------|------|------|-----------|
| `foo-app` | pod | 100h | AplicaÃ§Ã£o foo |
| `bar-app` | pod | 120h | AplicaÃ§Ã£o bar |
| `test-app` | pod | 80h | AplicaÃ§Ã£o de teste |
| `local-k8s-worker` | node | 500h | Worker node 1 |
| `local-k8s-worker2` | node | 500h | Worker node 2 |
| `local-k8s-control-plane` | control_plane | 800h | Control plane |

## ğŸš€ Como Usar

### 1. PrÃ©-requisitos

- Cluster Kubernetes funcionando (kind/minikube/etc)
- `kubectl` configurado
- Python 3.8+
- DependÃªncias: `numpy`, `pandas`, `matplotlib`

### 2. ExecuÃ§Ã£o via CLI

```bash
cd /home/jonascgn/Documentos/1_Artigo/testes

# SimulaÃ§Ã£o bÃ¡sica (24h fictÃ­cias, 1 iteraÃ§Ã£o)
python -m kuber_bomber.cli.availability_cli

# SimulaÃ§Ã£o personalizada (48h fictÃ­cias, 5 iteraÃ§Ãµes)
python -m kuber_bomber.cli.availability_cli --duration 48 --iterations 5

# SimulaÃ§Ã£o de 1 semana (168h fictÃ­cias) com delay customizado
python -m kuber_bomber.cli.availability_cli --duration 168 --iterations 10 --delay 30
```

**âš ï¸ IMPORTANTE - DuraÃ§Ã£o:**
- `--duration` Ã© em **HORAS FICTÃCIAS** (simuladas), nÃ£o tempo real
- Exemplo: `--duration 168` simula 1 semana de operaÃ§Ã£o em minutos reais
- O tempo real depende do nÃºmero de falhas e tempo de recuperaÃ§Ã£o

### 3. ParÃ¢metros

- `--duration`: DuraÃ§Ã£o da simulaÃ§Ã£o em **HORAS FICTÃCIAS** (padrÃ£o: 24)
- `--iterations`: NÃºmero de iteraÃ§Ãµes (padrÃ£o: 1)  
- `--delay`: Delay entre falhas em segundos **REAIS** (padrÃ£o: 60)

### 4. CritÃ©rios de Disponibilidade Interativos

O CLI pergunta para cada aplicaÃ§Ã£o quantos pods precisam estar funcionando:

```
ğŸ“¦ foo-app:
   Quantos pods de foo-app precisam estar Ready? (mÃ­n: 1): 2
   âœ… foo-app: mÃ­nimo 2 pod(s)

ğŸ“¦ bar-app:
   Quantos pods de bar-app precisam estar Ready? (mÃ­n: 1): 1
   âœ… bar-app: mÃ­nimo 1 pod(s)

ğŸ“¦ test-app:
   Quantos pods de test-app precisam estar Ready? (mÃ­n: 1): 3
   âœ… test-app: mÃ­nimo 3 pod(s)
```

**Sistema estÃ¡ disponÃ­vel quando:**
- `foo-app`: â‰¥ 2 pods Ready
- `bar-app`: â‰¥ 1 pod Ready  
- `test-app`: â‰¥ 3 pods Ready
- **E** todos os nodes estÃ£o funcionais
- **E** control plane estÃ¡ operacional

### 4. Uso ProgramÃ¡tico

```python
from kuber_bomber.simulation.availability_simulator import AvailabilitySimulator, Component

# Criar componentes customizados
components = [
    Component("my-app", "pod", mttf_hours=50.0),
    Component("worker-1", "node", mttf_hours=300.0)
]

# Criar simulador
simulator = AvailabilitySimulator(components=components, min_pods_required=1)

# Configurar critÃ©rios especÃ­ficos de disponibilidade
simulator.availability_criteria = {
    "my-app": 2,      # Precisa de pelo menos 2 pods da my-app
    "other-app": 1    # Precisa de pelo menos 1 pod da other-app
}

# Executar simulaÃ§Ã£o (12 horas fictÃ­cias, 3 iteraÃ§Ãµes)
simulator.run_simulation(duration_hours=12.0, iterations=3)
```

## ğŸ“Š RelatÃ³rios

Os relatÃ³rios sÃ£o salvos automaticamente em:

```
ano/mes/dia/component/availability_simulation/mttf_based/
â”œâ”€â”€ availability_simulation_YYYYMMDD_HHMMSS.csv    # Eventos detalhados
â””â”€â”€ simulation_stats_YYYYMMDD_HHMMSS.csv           # EstatÃ­sticas agregadas
```

### Formato dos Eventos

```csv
event_time_hours,real_time_seconds,component_type,component_name,
failure_type,recovery_time_seconds,system_available,available_pods,
required_pods,availability_percentage,downtime_duration,cumulative_downtime
```

### Formato das EstatÃ­sticas

```csv
metric,value,unit,description
simulation_duration_hours,24.0,hours,DuraÃ§Ã£o total da simulaÃ§Ã£o
total_failures,15,count,Total de falhas simuladas
system_availability,99.2,percentage,Disponibilidade geral do sistema
mean_recovery_time,45.3,seconds,Tempo mÃ©dio de recuperaÃ§Ã£o
total_downtime,0.8,hours,Tempo total de indisponibilidade
iterations_executed,1,count,NÃºmero de iteraÃ§Ãµes executadas
```

## ğŸ”§ LÃ³gica de Funcionamento

### 1. InicializaÃ§Ã£o
- Define componentes com seus MTTFs
- Cria fila de eventos ordenada por tempo
- Gera primeiro evento de falha para cada componente

### 2. Loop Principal
```python
while current_time < duration:
    event = heapq.heappop(event_queue)
    inject_failure(event.component)
    wait_for_recovery()  # Tempo real
    schedule_next_failure()  # +1min + exponential
    monitor_availability()
```

### 3. Timing HÃ­brido
- **Entre falhas**: 1 minuto fixo + intervalo exponencial
- **RecuperaÃ§Ã£o**: Tempo real atÃ© pods ficarem Ready
- **Monitoramento**: VerificaÃ§Ã£o contÃ­nua de disponibilidade

### 4. CritÃ©rio de Disponibilidade e CÃ¡lculo de Indisponibilidade

**Sistema estÃ¡ disponÃ­vel quando TODOS os critÃ©rios sÃ£o atendidos simultaneamente:**

**Pods por aplicaÃ§Ã£o (configurÃ¡vel):**
- `foo-app`: â‰¥ X pods Ready (usuÃ¡rio define X)
- `bar-app`: â‰¥ Y pods Ready (usuÃ¡rio define Y)  
- `test-app`: â‰¥ Z pods Ready (usuÃ¡rio define Z)

**Infraestrutura:**
- Nodes worker funcionais
- Control plane operacional

**â° CÃ¡lculo do Tempo de Indisponibilidade:**

O sistema calcula indisponibilidade baseado nos critÃ©rios especÃ­ficos:

```
Exemplo: fooâ‰¥2, barâ‰¥1, testâ‰¥3

ğŸŸ¢ DISPONÃVEL:    foo=3, bar=2, test=4  (todos critÃ©rios OK)
ğŸ”´ INDISPONÃVEL:  foo=1, bar=2, test=4  (foo abaixo do mÃ­nimo)
ğŸ”´ INDISPONÃVEL:  foo=2, bar=0, test=4  (bar abaixo do mÃ­nimo)  
ğŸ”´ INDISPONÃVEL:  foo=1, bar=0, test=2  (todos abaixo do mÃ­nimo)
```

**Algoritmo de cÃ¡lculo:**
1. A cada evento, verifica disponibilidade atual
2. Calcula tempo desde Ãºltima verificaÃ§Ã£o
3. Se sistema estava disponÃ­vel â†’ adiciona ao tempo_disponÃ­vel
4. Se sistema estava indisponÃ­vel â†’ adiciona ao tempo_indisponÃ­vel  
5. Disponibilidade% = (tempo_disponÃ­vel / tempo_total) Ã— 100

**Exemplo prÃ¡tico:**
- 10:00-10:30: todos OK â†’ 30min disponÃ­vel
- 10:30-10:35: foo cai para 1 â†’ 5min **indisponÃ­vel**
- 10:35-11:00: foo volta para 2 â†’ 25min disponÃ­vel  
- 11:00-11:10: test cai para 2 â†’ 10min **indisponÃ­vel**
- **Resultado:** 15min indisponÃ­vel de 70min total = 78.6% disponibilidade

## ğŸ“ˆ MÃ©tricas Coletadas

- **Disponibilidade do Sistema**: % de tempo que o sistema estÃ¡ disponÃ­vel
- **MTTR (Mean Time To Recovery)**: Tempo mÃ©dio de recuperaÃ§Ã£o
- **Downtime Total**: Tempo total de indisponibilidade
- **Falhas por Componente**: DistribuiÃ§Ã£o de falhas
- **EstatÃ­sticas de RecuperaÃ§Ã£o**: Min, max, mÃ©dia, desvio padrÃ£o

## ğŸ¯ Casos de Uso

1. **AnÃ¡lise de Confiabilidade**: Avaliar disponibilidade esperada da infraestrutura
2. **Planejamento de Capacidade**: Determinar nÃºmero mÃ­nimo de rÃ©plicas
3. **Teste de ResiliÃªncia**: Validar comportamento sob falhas
4. **OtimizaÃ§Ã£o de MTTF**: Encontrar pontos crÃ­ticos de falha
5. **Compliance SLA**: Verificar se sistema atende requisitos de disponibilidade

## ğŸ” LimitaÃ§Ãµes e ConsideraÃ§Ãµes

- SimulaÃ§Ã£o assume distribuiÃ§Ã£o exponencial (memoryless)
- Falhas sÃ£o independentes entre componentes
- Tempo de recuperaÃ§Ã£o Ã© medido em ambiente real
- Requer cluster Kubernetes funcional para testes completos
- SimulaÃ§Ã£o Ã© determinÃ­stica com seed fixo para reprodutibilidade

## ğŸš€ ExtensÃµes Futuras

- Suporte a correlaÃ§Ã£o entre falhas
- DistribuiÃ§Ãµes alternativas (Weibull, Normal)
- Interface web para visualizaÃ§Ã£o
- IntegraÃ§Ã£o com Prometheus/Grafana
- SimulaÃ§Ã£o de falhas de rede e armazenamento
- Modo "dry-run" para testes sem cluster real