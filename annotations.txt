=====================POD========================
Garantir que ta funcionando

Fazer um elemento

watch kubectl 

fazer um comando de falha
# Executa comando para matar processo principal (PID 1)

kubectl get pods
cmd = f"kubectl exec {pod_name} -- kill -9 1"

kubectl exec bar-app-6d4f4c8998-9mwxk -- kill -9 1

Reação default

Fazer uma tabela

================================================



=============================================
Fazer com que exiba o comando que foi executado

Utilizar os metodos existente de falha, criar uma classe qual que vou testar

Capturar para ver se realmente ta falhando, Falhar N(default=30) vezes um componente
(pod,processo(pod,worker_node,control plane),worker_node), e captura o tempo de recuperação(mttr),
entre cada execução espera M(default=60) segundos, deve gerar um csv com os tempos entre o inicio 
da falha e a recuperação, mais importante é o mttr

Cada aplicação e componente do kubernet tem um mttf e mttr diferente
=============================================


jonascgn@DESKTOP-HK185V6:~/Programas_Curso/1_Artigo$ kubectl get pods

=====================POD Antes========================
NAME                        READY   STATUS    RESTARTS   AGE
bar-app-6495f959f6-wktz9    1/1     Running   0          5m27s
foo-app-6898f5b49f-76c97    1/1     Running   0          5m27s
test-app-549846444f-pbsgl   1/1     Running   0          5m27s
======================================================

kubectl exec bar-app-6495f959f6-wktz9 -- sh -c ps aux
kubectl exec bar-app-6495f959f6-wktz9 -- sh -c 'kill -9 -1'

=====================POD Depois========================
jonascgn@DESKTOP-HK185V6:~/Programas_Curso/1_Artigo$ kubectl get pods
NAME                        READY   STATUS    RESTARTS     AGE
bar-app-6495f959f6-wktz9    0/1     Running   1 (4s ago)   7m50s
foo-app-6898f5b49f-76c97    1/1     Running   0            7m50s
test-app-549846444f-pbsgl   1/1     Running   0            7m50s
======================================================

=====================POD Depois de um tempo===========
NAME                        READY   STATUS    RESTARTS       AGE
bar-app-6495f959f6-wktz9    1/1     Running   1 (4m6s ago)   11m
foo-app-6898f5b49f-76c97    1/1     Running   0              11m
test-app-549846444f-pbsgl   1/1     Running   0              11m
======================================================

=====================POD Depois de um tempo===========
NAME                        READY   STATUS    RESTARTS      AGE
bar-app-6495f959f6-wktz9    0/1     Running   2 (23s ago)   20m
foo-app-6898f5b49f-76c97    1/1     Running   0             20m
test-app-549846444f-pbsgl   1/1     Running   0             20m
======================================================

=====================POD Depois de um tempo===========
NAME                        READY   STATUS    RESTARTS       AGE
bar-app-6495f959f6-wktz9    1/1     Running   2 (4m6s ago)   24m
foo-app-6898f5b49f-76c97    1/1     Running   0              24m
test-app-549846444f-pbsgl   1/1     Running   0              24m
======================================================

curl http://localhost:8080/foo


============2025/10/21================

Coisas para fazer na versão final:

- Definir o mttr manualmente de um componente como vm ou aplicação
- 

Coisas para fazer agora:

- Criar lista de eventos de falha

- Listar Todos Componentes da Infraestrutura(Pods, Nodes, VMs, Aplicações) ->
  Pegar mttf(valor(ex:800h)) de cada componente e gerar uma falha exponencialmente distribuída com base no mttf de cada componente -> 
  Pra cada componente que vai falhar na lista, vai pegar o que falha mais proximo falha o componente, 
  Observa o tempo que demorou pra recuperar(mttr) e depois que se recupera gera um novo tempo de falha pro componente com base no mttf 
  Verificar os logs e verificar o criterios de disponibildade apartir de quantos pods funcionarao e ver a intersecao
  entre todos os pods que estao rodando e os que falharam, quanto tempo o sistema ficou indisponivel e quanto tempo ficou disponivel
  Gerar um relatorio final com base nesses dados

- Executar esse processo N vezes

- Trabalhar com tempo de redução(2880h -> 5minutos, 34000x menor), toda vez que o componente se recupera, ele espera 1min(delay)
    e depois falha o proximo  dependendo do tempo de recuperacao

- Tempo que espera nao importa sera 1min e computo 8...
- Tempo que espera nao importa sera 1min e computo 300...

- Os que nao for self-healing, espera 1min